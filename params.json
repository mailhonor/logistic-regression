{"name":"Logistic-regression","tagline":"逻辑回归by bgd and sgd","body":"关于LR（线性回归），有蛮多自己的思考的，现在一一记录下来，作为感悟。\r\n线性回归简单看来，就是最小化损失函数。损失函数可以有很多的形式，比如：Loss = sum[ (f(xi) - y)^2 for i in (1, n) ] 这样的形式是符合最大似然的形式。\r\n假设f(xi) = wx+b的形式，即线性的形式，那么 Loss = sum[ (wx+b-y)^2 for i in (1, n) ]，如果采用sgd的优化方法，对其求偏导得到：\r\n对w求偏导：Loss'w = (wx+b-y)x\r\n对b求偏导：Loss'b = (wx+b-y)\r\n那么得到更新公式：\r\nw' = w + Loss'w*alpha\r\nb' = b + Loss'b*alpha\r\n其中，alpha是学习率，公式化简即：\r\nw' = w + (wx+b-y)x*alpha\r\nb' = b + (wx+b-y)*alpha\r\n考虑多个feature的情况，就是：\r\nw1' = w1 + (wx+b-y)x1*alpha\r\nw2' = w2 + (wx+b-y)x2*alpha\r\n...\r\nwn' = wn + (wn+b-y)xn*alpha\r\n其中，b'可以看做xi=1的情况。\r\n考虑成为向量的情况，\r\nw = w - (wx+b-y)*x*alpha\r\n\r\n关于LR（逻辑回归），即在线性回归的基础之上，加入了逻辑函数sigmoid函数，也是DNN中的激活函数，也即我的英文名的由来，函数为：\r\nLoss = sum[ yi*log(yi-f(xi) + (1-yi)*log(f(xi)) ] for i in (1, n)\r\n它是符合sigmoid到yi的距离作为概率p的最大似然，这里简单验证一下，\r\nsigmoid(x) = 1 / ( 1+ e^-x )\r\nlikelihood = product[ yi*(yi-f(xi) + (1-yi)*f(xi) ] for i in (1,n)\r\n取log之后有，\r\nloglikelihood = sum[ yi*log(yi-f(xi) + (1-yi)*log(f(xi))] for i in (1, n)\r\n接下来和线性回归一样的方法，直接考虑多个feature的情况：\r\nlikelihood = sum[ yi*log(sigmoid(wx))+(1-yi)*log(1-sigmoid(wx)) ] for i in (1,n)\r\n这个推倒还是蛮复杂的，最终结果是：\r\nlikelihood'wi = (y-f(x))*x\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}